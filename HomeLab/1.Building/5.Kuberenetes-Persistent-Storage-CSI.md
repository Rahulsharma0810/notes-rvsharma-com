
Welcome to my HomeLab! In this post, I’ll walk you through my journey of setting up a Kubernetes storage solution. I explored two popular options: **local-path-provisioner** and **Ceph Rook**. While both have their strengths, I opted for **Ceph Rook** instead of simple **local-path-provisioner** because of native snapshots and mirroring.

Complete source code for the live cluster is available https://github.com/Rahulsharma0810/HomeLab-Kubernetes

## My Current Setup

Hardware

| SSD | M.2 Predator SSD GM7000 2TB<br>2 X Samsung SSD 860 250GB<br> |
| --- | ------------------------------------------------------------ |

Storage Alignment Across VMs 

| **VM**        | **Disk**                             |
| ------------- | ------------------------------------ |
| Proxmox       | 100 GB                               |
| Control-Plane | 50 GB (For Only K 8 s  Components)   |
| Worker Node   | 200 GB                               |
| Worker Node   | 1600 GB (Raw Disk, Shared From 2 TB) |
| Worker Node   | 2 X Samsung SSD 860 250GB            |
Attaching Disk to Worker Node via promox.
```
ls -n /dev/disk/by-id/
qm set 101 -virtio2 /dev/disk/by-id/wwn-0x5002538e40eb13c9
qm set 101 -virtio3 /dev/disk/by-id/wwn-0x5002538e40eb1435
```

Below is the Talos Linux View of Worker Node.
```
❯ talosctl get disks -n 192.168.0.6
NODE          NAMESPACE   TYPE   ID      VERSION   SIZE     READ ONLY   TRANSPORT   ROTATIONAL   WWID   MODEL           SERIAL
192.168.0.6   runtime     Disk   loop0   1         75 MB    true
192.168.0.6   runtime     Disk   sda     1         215 GB   false       virtio      true                QEMU HARDDISK
192.168.0.6   runtime     Disk   sr0     1         106 MB   false       ata         true                QEMU DVD-ROM
192.168.0.6   runtime     Disk   vda     1         1.6 TB   false       virtio      true
192.168.0.6   runtime     Disk   vdb     1         250 GB   false       virtio      true
192.168.0.6   runtime     Disk   vdc     1         250 GB   false       virtio      true
```
## Main Goal

- VDA - Storing all the PVs in Worker Node's 1.6 TB.
- VDB - Using as Backup Disk For Critical workloads, Like PaperlessNgx, Immich etc.
- VDC - Using as 3 rd point of backups, in case VDA, VDB fails.

Note: Not all K8s PVs will be synced in VDB and VDC, Only critical workloads. Need to find a way to explicitly snapshot some PVs, For example, Immich, Papelessngx etc.

Incase, VDA failed i can reinstall cluster / application and then pull data data from VDB and VDC.

In the end i started using local-path-provisioner for easy management since its 1 node cluster.

---
## **Ceph Rook: Scalable and Reliable Storage**

Ceph Rook provides enterprise-grade storage with redundancy, scalability, and seamless Kubernetes integration. While it’s a robust solution, setting it up in a single-node cluster presented some unique challenges.

Checkout the live configurations at : https://github.com/Rahulsharma0810/HomeLab-Kubernetes

### Installing Ceph RooK

Follow Flux CD Structure for installing rook and ceph at  https://github.com/Rahulsharma0810/HomeLab-Kubernetes

### Deploying Ceph tools

First get secret

```
❯ k describe secrets rook-ceph-mon
Name:         rook-ceph-mon
Namespace:    rook-ceph
Labels:       <none>
Annotations:  <none>

Type:  kubernetes.io/rook

Data
====
ceph-secret:    40 bytes
ceph-username:  12 bytes
fsid:           36 bytes
mon-secret:     40 bytes

❯ k get secrets rook-ceph-mon -o "jsonpath={.data['ceph-secret']}" | base64 -D
XX+XX==%
❯ k get secrets rook-ceph-mon -o "jsonpath={.data['fsid']}" | base64 -D
3951c4e9-XX-47f4-ac9d-XX%
```

Declare secret in ConfigMap.

```
# HomeLab-Kubernetes/manifests/infrastructure/ceph-cluster/toolbox.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: rook-ceph-config
  namespace: rook-ceph
data:
  ceph.conf: |
    [global]
    fsid = 3951c4e9-XX-47f4-ac9d-XX
    mon_host = 192.168.0.6
    auth_cluster_required = cephx
    auth_service_required = cephx
    auth_client_required = cephx
  ceph.client.admin.keyring: |
    [client.admin]
        key = XX+XX==
        caps mds = "allow *"
        caps mon = "allow *"
        caps osd = "allow *"
        caps mgr = "allow *"
```

Let's get into the tools pod and verify ceph status.

```
[root@rook-ceph-tools-844b49ddc-58jln /]# ceph status
  cluster:
    id:     3951c4e9-61aa-47f4-ac9d-14a56673a4d5
    health: HEALTH_WARN
            1 MDSs report slow metadata IOs
            Reduced data availability: 4 pgs inactive
            OSD count 0 < osd_pool_default_size 1

  services:
    mon: 1 daemons, quorum a (age 15m)
    mgr: a(active, since 15m)
    mds: 1/1 daemons up, 1 standby
    osd: 0 osds: 0 up, 0 in

  data:
    volumes: 1/1 healthy
    pools:   4 pools, 4 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:     100.000% pgs unknown
             4 unknown

[root@rook-ceph-tools-844b49ddc-58jln /]# ceph osd tree
ID  CLASS  WEIGHT  TYPE NAME     STATUS  REWEIGHT  PRI-AFF
-1              0  root default
```

### Ceph Web Dashboard 

I have exposed the url for better observabilty with CF Tunnels, See main repo for tunnel confs.

```
kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo
```

### Issues & Solutions

#### No OSDs
Ceph Cluster keep giving me Warn, inspite the disks were available to the node.

Right now i am at-least keep trying this 40 th time to fix the error, lol.

Cpeh wants a wiped disk without partitions else it keep showing warning and note disk as OSds.

Below are the different tried i have done during the exercise. 

##### Wipe: Talos Method

```
talosctl reset -n 192.168.0.6 --user-disks-to-wipe /dev/vda,/dev/vdb,/dev/vdc --reboot
```

Node got stopped after this command, had to restart from proxmox again and change the ip, also again applied the node configuration with

```
talosctl apply-config --insecure --nodes 192.168.0.6 --file rendered/worker01.yaml
```

Below doesn't help
```
talosctl disks wipe /dev/vda -n 192.168.0.6
```

##### Wipe: Gparted GUI ISO

booted gparted gui iso to wipe these disk out and then started vm with talos linux iso.

![[attachments/2024-12-17_02-15-41.png]]

##### Wipe: BusyBox pod

Running Command in other pod.
```
  # HomeLab-Kubernetes/manifests/infrastructure/ceph-cluster/ceph-diskwipe.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: disk-wiper
    namespace: rook-ceph
  spec:
    restartPolicy: Never
    containers:
    - name: disk-wiper
      image: busybox
      securityContext:
        privileged: true
        allowPrivilegeEscalation: true
      command:
      - "/bin/sh"
      - "-c"
      - |
        echo "Wiping disks vda, vdb, and vdc..."
        dd if=/dev/zero of=/dev/vda bs=1M count=100 || true
        dd if=/dev/zero of=/dev/vdb bs=1M count=100 || true
        dd if=/dev/zero of=/dev/vdc bs=1M count=100 || true
        echo "Disk wipe completed."
```

Lastly the Issue was with the manifest. (Commented out code was referred via ChatGPT and it is completely wrong, Cluster warn issue solved by removing the commented code.)
```
useAllDevices: true
# nodes:
# - name: I1-1806-Talos-Worker01
#   devices:
#   - name: vda
#   - name: vdb
#   - name: vdc
nodes:
- config:
    osdsPerDevice: "1"
    storeType: bluestore
```

#### HEALTH_WARN: Reduced data availability: 32 pgs inactive, Degraded data redundancy: 32 pgs undersized

As soon as we applied replicatd.Size = 3, Cluster goes to Warn Mode.

File can be read at /manifests/infrastructure/ceph-cluster/replicated-x3-block-store.yaml

```
[root@rook-ceph-tools-0 /]# ceph osd df
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP     META     AVAIL    %USE  VAR   PGS  STATUS
 0    hdd  1.46480   1.00000  1.5 TiB   76 MiB   10 MiB    1 KiB   65 MiB  1.5 TiB  0.00  0.67   92      up
 1    hdd  0.22739   1.00000  233 GiB   46 MiB  1.9 MiB    1 KiB   44 MiB  233 GiB  0.02  2.58   15      up
 2    hdd  0.22739   1.00000  233 GiB   28 MiB  1.8 MiB    1 KiB   26 MiB  233 GiB  0.01  1.58    6      up
                       TOTAL  1.9 TiB  150 MiB   14 MiB  4.7 KiB  135 MiB  1.9 TiB  0.01
MIN/MAX VAR: 0.67/2.58  STDDEV: 0.01
```

```
[root@rook-ceph-tools-0 /]# ceph pg stat
113 pgs: 32 undersized+peered, 81 active+clean; 32 MiB data, 150 MiB used, 1.9 TiB / 1.9 TiB avail; 1.2 KiB/s rd, 2 op/s
```

**Why is This Happening?**

- The replication factor (`size: 3`) requires three copies of each piece of data across different OSDs.
- **Smaller disks** (`vdb` and `vdc`) cannot hold the same amount of data as `vda`, leading to an imbalance

```
ceph osd pool set replicated-x3-block-store size 2
```

Solved the Error and created another below issues. :)

###### Health Warn: Degraded data redundancy: 16/146 objects degraded (10.959%), 11 pgs degraded, 24 pgs undersized

After degrading below is the new issue.

```
Degraded data redundancy: 16/146 objects degraded (10.959%), 11 pgs degraded, 24 pgs undersized
```

Tried Ceph to re-caluculate disk weights.

```
[root@rook-ceph-tools-0 /]# ceph osd reweight-by-utilization
Error EDOM: Refusing to reweight: we only have 158432 kb used across all osds!
FAILED reweight-by-pg
```

```
[root@rook-ceph-tools-0 /]# ceph osd df
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP     META     AVAIL    %USE  VAR   PGS  STATUS
 0    hdd  1.46480   1.00000  1.5 TiB   82 MiB   16 MiB    1 KiB   65 MiB  1.5 TiB  0.01  0.61   92      up
 1    hdd  0.22739   1.00000  233 GiB   49 MiB  5.0 MiB    1 KiB   44 MiB  233 GiB  0.02  2.33   21      up
 2    hdd  0.22739   1.00000  233 GiB   46 MiB  2.0 MiB    1 KiB   44 MiB  233 GiB  0.02  2.19    8      up
                       TOTAL  1.9 TiB  177 MiB   23 MiB  4.7 KiB  153 MiB  1.9 TiB  0.01
```

The error message indicates that Ceph refuses to run `reweight-by-utilization` because **the overall cluster usage is very low** (only 158 MiB used). This command is generally for significant imbalances when substantial data is stored. Since your cluster is mostly empty, Ceph doesn't see a need to reweight.

After reading, I found custom crushmap can be create to target only vda for x 1, and another crush rule for x 3 to taget all disks. 

And in cephBlockPool, 

`parameters. Crush_rule: custom_crushmap_for_vda `

Can be added, Perhaps i kept getting error, so i have switched to **local-path-provisioner**.

---
## **Local Path Provisioner**

The **local-path-provisioner** is a lightweight and simple storage solution for Kubernetes. It’s perfect for single-node clusters where compute and resource efficiency are priorities. But like any solution, it has its trade-offs.

### **Pros**
- **Minimal Components**: Requires very little overhead, ideal for HomeLabs.
- **Fast Performance**: Direct access to the node’s local storage ensures excellent performance.
### **Cons**
1. **No Built-In Backup and Recovery**:
   - You’ll need to configure backups manually or rely on app-native solutions. Luckily, many modern applications support backup and restore. If not, a simple cron job can work wonders.
2. **No Web UI**:
   - Monitoring storage usage requires CLI tools; no visual dashboard is available.
3. **Privileged Mode Concerns**:
   - The provisioner relies on `hostPath` and privileged mode, which can:
     - Increase attack vectors (e.g., privilege escalation).
     - Open the host filesystem to unintended modifications.
     - Bypass Kubernetes security policies, such as `PodSecurityAdmission`.

**Quick Notes to add Disk in talos Linux to use with local-path-provisioner.**
- Open `gparted`, select the disk from the dropdown in the top right.
- In the menu, select `Device` -> `Create Partition Table` -> `Type GPT`.
- Then, select `Partition`.
- In the menu, leave everything as default, except `File system` -> `XFS`.

Local-path-provisionar doesn't support multiple storage classes with specific, hostpath, so i installed 3 Local-path-provisionar  for three storage classes, specified to each hostpath of 3 disks.

See file at manifests/infrastructure/local-path-provisioner/release.yaml

