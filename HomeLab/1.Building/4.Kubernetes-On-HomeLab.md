## Choosing a Platform for Kubernetes

Although Ubuntu Minimal Server serves the purpose, I do not want to allocate additional compute resources to the OS. For instance, if I have 1 master + 4 nodes, each running Ubuntu, there will be unnecessary compute usage, ranging from 4–10% depending on the load.

I discovered another CNCF-backed project called **Talos Linux**, which is a minimal OS specifically designed for Kubernetes, sized at around 120MB, and does not include tools like SSH. Think of it as a lightweight, secure Kubernetes appliance.

### Talos Linux on Proxmox

100 GB - Proxmox OS.
50 GB - Talos Master (No Workloads).
200 GB - Talos Worker.
150 GB - Other VMs.

| **Talos Linux Node** | **VM Count** | **CPU**                      | **Memory** | **Disk**          | **IP Address** |
| -------------------- | ------------ | ---------------------------- | ---------- | ----------------- | -------------- |
| Master v1.8.3        | 1            | 1 Socket, 16 Cores, Host CPU | 6GB        | 50GB (local-lvm)  | 192.168.0.5    |
| Worker v1.8.3        | 1            | 1 Socket, 16 Cores, Host CPU | 48GB       | 200GB (local-lvm) | 192.168.0.6    |

I downloaded the ISO with the following configuration:

I did not enable Secure Boot as it required kernel flags that I was not sure about.

**URL:** [Talos Linux ISO](https://factory.talos.dev/?arch=amd64&board=undefined&cmdline-set=true&extensions=-&extensions=siderolabs%2Fqemu-guest-agent&platform=metal&target=metal&version=1.8.2)

```bash
customization:
    systemExtensions:
        officialExtensions:
            - siderolabs/qemu-guest-agent
```

 **Steps Followed**

1. Disable Secure Boot and start the VM.
2. Assign static IP via interactive serial console.
   
Guide reference: [Talos Linux on Proxmox](https://www.talos.dev/v1.8/talos-guides/install/virtualized-platforms/proxmox/)

#### Generate Secrets and Configuration

```
talosctl gen secrets --output-file secrets.yaml
```

```bash
export API_ENDPOINT="https://192.168.0.5:6443"
export CLUSTER_NAME="talos-proxmox-cluster"

talosctl gen config \
     --with-secrets secrets.yaml \
     --output-types talosconfig \
     --output talosconfig \
     $CLUSTER_NAME \
     $API_ENDPOINT --install-image factory.talos.dev/installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.8.2 --force
```

#### Talos Configurations

### Configuration for CNI

```yaml
# patches/disable-kube-proxy-and-cni.yaml
cluster:
  network:
    cni:
      name: none
  proxy:
    disabled: true
```

### Assign Static IPs to Control Plane and Worker Nodes

```yaml
# HomeLab-Kubernetes/nodes/controlplane.yaml
machine:
  network:
    hostname: I1-1806-Talos-CP01
    interfaces:
    - interface: ens18
      dhcp: false
      addresses:
      - 192.168.0.5/24
      routes:
      - network: 0.0.0.0/0
        gateway: 192.168.0.1
```

**Adding External Disks and GPU Label to the Node.**

⚠️ Make Sure, to follow [[3.Setting-Up-Graphic-Card]], Before making labels. 
```yaml
# nodes/worker01.yaml
machine:
  network:
    hostname: I1-1806-Talos-Worker01
    interfaces:
    - interface: ens18
      dhcp: false
      addresses:
      - 192.168.0.6/24
      routes:
      - network: 0.0.0.0/0
        gateway: 192.168.0.1
  kubelet:
    extraMounts:
    - destination: /var/mnt/ssd_nvme_predator_GM7000
      type: bind
      source: /var/mnt/ssd_nvme_predator_GM7000
      options:
      - bind
      - rshared
      - rw
    - destination: /var/mnt/ssd_sata_samsung_860_evo_1
      type: bind
      source: /var/mnt/ssd_sata_samsung_860_evo_1
      options:
      - bind
      - rshared
      - rw
    - destination: /var/mnt/ssd_sata_samsung_860_evo_2
      type: bind
      source: /var/mnt/ssd_sata_samsung_860_evo_2
      options:
      - bind
      - rshared
      - rw
    - destination: /var/mnt/ssd_data_Lexar_120GB_1
      type: bind
      source: /var/mnt/ssd_data_Lexar_120GB_1
      options:
      - bind
      - rshared
      - rw
    - destination: /var/mnt/ssd_data_Lexar_120GB_2
      type: bind
      source: /var/mnt/ssd_data_Lexar_120GB_2
      options:
      - bind
      - rshared
      - rw
  disks:
  - device: /dev/vda
    partitions:
    - mountpoint: /var/mnt/ssd_nvme_predator_GM7000
  - device: /dev/vdb
    partitions:
    - mountpoint: /var/mnt/ssd_sata_samsung_860_evo_1
  - device: /dev/vdc
    partitions:
    - mountpoint: /var/mnt/ssd_sata_samsung_860_evo_2
  - device: /dev/vdd
    partitions:
    - mountpoint: /var/mnt/ssd_data_Lexar_120GB_1
  - device: /dev/vde
    partitions:
    - mountpoint: /var/mnt/ssd_data_Lexar_120GB_2
  kernel:
    modules:
    - name: nvidia
    - name: nvidia_uvm
    - name: nvidia_drm
    - name: nvidia_modeset
  sysctls:
    net.core.bpf_jit_harden: 1
  nodeLabels:
    feature.node.kubernetes.io/pci-10de.present: "true"
```

**Configure Cluster Name**

```yaml
cluster:
  name: talos-proxmox-cluster
```

### Generate Final Files

**Controlplane.yaml**

```bash
export API_ENDPOINT="https://192.168.0.5:6443"
export CLUSTER_NAME="talos-proxmox-cluster"
talosctl gen config \
        --output rendered/controlplane.yaml                       \
        --output-types controlplane                               \
        --with-cluster-discovery=false                            \
        --with-secrets secrets.yaml                               \
        --config-patch @patches/cluster-name.yaml                 \
        --config-patch @patches/disable-kube-proxy-and-cni.yaml   \
        --config-patch @nodes/controlplane.yaml                   \
        $CLUSTER_NAME                                             \
        $API_ENDPOINT
```

**Worker1.yaml**

```bash
export API_ENDPOINT="https://192.168.0.5:6443"
export CLUSTER_NAME="talos-proxmox-cluster"
talosctl gen config \
	    --output rendered/worker01.yaml \
	    --output-types worker \
	    --with-cluster-discovery=false \
	    --with-secrets secrets.yaml \
	    --config-patch @patches/cluster-name.yaml \
	    --config-patch @patches/disable-kube-proxy-and-cni.yaml \
	    --config-patch @nodes/worker01.yaml \
	    $CLUSTER_NAME \
	    $API_ENDPOINT
```

Now we have two files generated

```bash
├── rendered
│   ├── controlplane.yaml
│   └── worker01.yaml
```

### Installing Kubernetes

```bash
export TALOSCONFIG=./talosconfig
talosctl apply-config --insecure --nodes 192.168.0.5 --file rendered/controlplane.yaml
talosctl apply-config --insecure --nodes 192.168.0.6 --file rendered/worker01.yaml
```

#### BootStrap Cluster

```bash
talosctl config endpoint 192.168.0.5
talosctl config node 192.168.0.5
talosctl bootstrap
talosctl kubeconfig .
rm -rf ~/.kube/config
cp kubeconfig ~/.kube/config
cp talosconfig ~/.talos/config
```

Wait for Nodes to Reboot, For Troubleshooting attach talos serial dashboard.

```bash
talosctl -n 192.168.0.5 dashboard
```

Apply Change in Cluster Lifecycle for example, adding disks etc.

```
talosctl -n 192.168.0.6 apply machineconfig -f rendered/worker01.yaml
```
## Install Cilium

Without CNI, Node do not get ready.

```bash
helm install                                                    \
    cilium                                                      \
    cilium/cilium                                               \
    --namespace kube-system                                     \
    --set ipam.mode=kubernetes                                  \
    --set hostFirewall.enabled=false                            \
    --set bpf.masquerade=true                                   \
    --set ipMasqAgent.enabled=true                              \
    --set hubble.relay.enabled=true                             \
    --set hubble.ui.enabled=true                                \
    --set kubeProxyReplacement=true                             \
    --set securityContext.capabilities.ciliumAgent="{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}" \
    --set securityContext.capabilities.cleanCiliumState="{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}" \
    --set cgroup.autoMount.enabled=false                        \
    --set cgroup.hostRoot=/sys/fs/cgroup                        \
    --set k8sServiceHost=localhost \
    --set k8sServicePort=7445
```

Check Status

```bash
❯ k get pods -A
NAMESPACE     NAME                                         READY   STATUS    RESTARTS      AGE
kube-system   cilium-97bvm                                 1/1     Running   0             11m
kube-system   cilium-9jcqm                                 1/1     Running   0             11m
kube-system   cilium-envoy-h4m8v                           1/1     Running   0             11m
kube-system   cilium-envoy-vmkml                           1/1     Running   0             11m
kube-system   cilium-operator-54ff457fd7-wssmn             1/1     Running   0             11m
kube-system   cilium-operator-54ff457fd7-x9h2h             1/1     Running   0             11m
kube-system   coredns-68d75fd545-7czmm                     1/1     Running   0             13h
kube-system   coredns-68d75fd545-xsd59                     1/1     Running   0             13h
kube-system   hubble-relay-c56665db6-4vnl6                 1/1     Running   0             11m
kube-system   hubble-ui-77555d5dcf-qnk7p                   2/2     Running   0             11m
kube-system   kube-apiserver-i1-1806-talos-cp01            1/1     Running   0             12h
kube-system   kube-controller-manager-i1-1806-talos-cp01   1/1     Running   2 (12h ago)   12h
kube-system   kube-scheduler-i1-1806-talos-cp01            1/1     Running   2 (12h ago)   12h

```

If not then again check the Serial From above command `talosctl -n 192.168.0.5 dashboard`

**Get Kubelet Status**

```bash
talosctl -n 192.168.0.5 service kubelet
```

Get Containers

```bash
talosctl -n 192.168.0.5 containers --kubernetes
```

## Tip: Reboot Nodes

```bash
talosctl reboot --nodes 192.168.0.5
talosctl reboot --nodes 192.168.0.6
```

## Firewall

### Policy Audit Mode

```bash
FIRST_CILIUM_POD=$(kubectl -n kube-system get pods -l "k8s-app=cilium" -o jsonpath='{.items[0].metadata.name}')
echo $FIRST_CILIUM_POD
ENDPOINT_ID=$(kubectl exec -n kube-system $FIRST_CILIUM_POD -- cilium endpoint list -o jsonpath='{[?(@.status.identity.id==1)].id}')
echo $ENDPOINT_ID

kubectl exec -n kube-system $FIRST_CILIUM_POD -- cilium endpoint config $ENDPOINT_ID | grep PolicyAuditMode

### Enable Audit Mode

kubectl exec -n kube-system $FIRST_CILIUM_POD -- cilium endpoint config $ENDPOINT_ID PolicyAuditMode=Enabled

### Recheck

kubectl exec -n kube-system $FIRST_CILIUM_POD -- cilium endpoint config $ENDPOINT_ID | grep PolicyAuditMode
```

```bash
k apply -f manifests/infrastructure/cluster-policies/host-fw-control-plane.yaml

### Check Logs
kubectl exec -n kube-system $FIRST_CILIUM_POD -- cilium monitor -t policy-verdict --related-to $ENDPOINT_ID
```

## FluxCD

```bash
export GITHUB_USER=Rahulsharma0810
export GITHUB_TOKEN=github_pat_XXX
```

```bash
flux bootstrap github --owner=$GITHUB_USER --repository=HomeLab-Kubernetes --branch=main --path=./manifests/cluster --personal
```

## Testing Things

```bash
k apply -f manifests/tests/podinfo.yaml
```

```bash
k -n default port-forward podinfo-59f9698799-nwmlw 9898
```

Test the Ping in the browser.

## Backporting Cilium

Directory Structure for commit.

```

└── manifests
    ├── cluster
    │   └── flux-system
    │       ├── cilium.yaml             < Commmitted
    │       ├── gotk-components.yaml
    │       ├── gotk-sync.yaml
    │       └── kustomization.yaml      < Commmitted
    ├── infrastructure
    │   ├── cilium                      < Commmitted
    │   │   ├── kustomization.yaml
    │   │   ├── release.yaml
    │   │   └── repository.yaml
    │   └── cluster-policies
    │       ├── host-fw-control-plane.yaml
    │       └── kustomization.yaml
```

### Error

{peerTarget:hubble-peer.kube-system.svc.cluster.local:443 dialTimeout:30000000000}

Not Sure, Why install cilium with hostFirewall disable, and then installing it via flux with hostFirewall enable fixes the pod healthcheck.